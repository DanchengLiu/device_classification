{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XGHPnr95xZ5c"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities Functions"
      ],
      "metadata": {
        "id": "XGHPnr95xZ5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_from_full_dataset(full_dataset_path, capture_date,rx_name,prefix=None):\n",
        "    src=full_dataset_path\n",
        "\n",
        "    if prefix is None: \n",
        "        dataset_path = '{}pkl_wifi_{}/dataset_{}_node{}.pkl'.format(src,capture_date,capture_date,rx_name)\n",
        "    else:\n",
        "        dataset_path = '{}pkl_wifi_{}_{}/dataset_{}_node{}.pkl'.format(src,prefix,capture_date,capture_date,rx_name)\n",
        "   \n",
        "    if os.path.isfile(dataset_path) :\n",
        "        with open(dataset_path,'rb') as f:\n",
        "            dataset = pickle.load(f)\n",
        "    else:\n",
        "            dataset = None\n",
        "#             print('Not Found')\n",
        "#             print(dataset_path)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_compact_pkl_dataset(dataset_path,dataset_name):\n",
        "    with open(dataset_path+dataset_name+'.pkl','rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def shuffle(vec1,vec2,seed = 0):\n",
        "    np.random.seed(0)\n",
        "#     print(vec1.shape[0],vec2.shape[0])\n",
        "    shfl_indx = np.arange(vec1.shape[0])\n",
        "    np.random.shuffle(shfl_indx)\n",
        "    shfl_indx = shfl_indx.astype('int')\n",
        "    vec1 = vec1[shfl_indx]\n",
        "    vec2 = np.copy(vec2[shfl_indx])\n",
        "    return vec1,vec2\n",
        "\n",
        "\n",
        "def norm(sig_u):\n",
        "    if len(sig_u.shape)==3:\n",
        "        pwr = np.sqrt(np.mean(np.sum(sig_u**2,axis = -1),axis = -1))\n",
        "        sig_u = sig_u/pwr[:,None,None]\n",
        "    if len(sig_u.shape)==2:\n",
        "        pwr =  np.sqrt(np.mean(np.sum(sig_u**2,axis = -1),axis = -1))\n",
        "        sig_u = sig_u/pwr\n",
        "    # print(sig_u.shape)\n",
        "    return sig_u\n",
        "\n",
        "def split3(vec,n1,n2):\n",
        "    vec1 = vec[0:n1]\n",
        "    vec2 = vec[n1:n1+n2]\n",
        "    vec3 = vec[n1+n2:]\n",
        "    return vec3,vec1,vec2\n",
        "\n",
        "def split_set3(st,f1,f2):\n",
        "    [sig,txid] = st\n",
        "\n",
        "    n_samples  = sig.shape[0]\n",
        "    n1 = int(f1*n_samples)\n",
        "    n2 = int(f2*n_samples)\n",
        "\n",
        "    sig1,sig2,sig3 = split3(sig,n1,n2)\n",
        "    txid1,txid2,txid3 = split3(txid,n1,n2)\n",
        "    st1 = [sig1,txid1]\n",
        "    st2 = [sig2,txid2]\n",
        "    st3 = [sig3,txid3]\n",
        "    return st1,st2,st3 \n",
        "\n",
        "def get_node_indices(tx_name_list,node_name_list):\n",
        "    op_list = []\n",
        "    for tx in tx_name_list:\n",
        "        if tx in node_name_list:\n",
        "            op_list.append(node_name_list.index(tx))\n",
        "        else:\n",
        "            op_list.append(None)\n",
        "    return op_list\n",
        "    \n",
        "def parse_nodes(dataset,node_list,seed = 0):\n",
        "    cat_sig = []\n",
        "    cat_txid = []\n",
        "    data = dataset['data']\n",
        "    \n",
        "    \n",
        "    for i,node in enumerate(node_list):\n",
        "        if (not node  is  None) and  node < len(data):\n",
        "            cat_sig.append(data[node])\n",
        "            cat_txid.append(np.ones( (data[node].shape[0]) )*i)\n",
        "    cat_sig = np.concatenate(cat_sig)\n",
        "    cat_txid = np.concatenate(cat_txid)\n",
        "    np.random.seed(seed)\n",
        "    cat_sig,cat_txid = shuffle(cat_sig,cat_txid)\n",
        "    cat_sig = norm(cat_sig)\n",
        "    return (cat_sig,cat_txid)\n",
        "\n",
        "def to_categorical(y, num_classes=None, dtype='float32'):\n",
        "    y = np.array(y, dtype='int')\n",
        "    input_shape = y.shape\n",
        "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
        "        input_shape = tuple(input_shape[:-1])\n",
        "    y = y.ravel()\n",
        "    if not num_classes:\n",
        "        num_classes = np.max(y) + 1\n",
        "    n = y.shape[0]\n",
        "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
        "    categorical[np.arange(n), y] = 1\n",
        "    output_shape = input_shape + (num_classes,)\n",
        "    categorical = np.reshape(categorical, output_shape)\n",
        "    return categorical\n",
        "\n",
        "def prepare_txid_and_weights(st,n):\n",
        "    sig,txid = st\n",
        "    txid_oh = to_categorical(txid,n)\n",
        "    stat= np.sum(txid_oh,axis=0)\n",
        "    cls_weights = np.max(stat,axis=0)/stat \n",
        "    cls_weights = cls_weights.tolist()\n",
        "    augset = [sig,txid,txid_oh,cls_weights]\n",
        "    return augset\n",
        "\n",
        "def prepare_dataset(dataset,tx_name_list,val_frac=0.1, test_frac=0.1):\n",
        "    tx_list = get_node_indices(tx_name_list,dataset['node_list'])\n",
        "    all_set = parse_nodes(dataset,tx_list,seed = 0)\n",
        "    train_set,val_set,test_set = split_set3(all_set,val_frac, test_frac)\n",
        "    train_augset = prepare_txid_and_weights(train_set,len(tx_list))\n",
        "    val_augset = prepare_txid_and_weights(val_set,len(tx_list))\n",
        "    test_augset = prepare_txid_and_weights(test_set,len(tx_list))\n",
        "    return train_augset,val_augset,test_augset\n",
        "\n",
        "\n",
        "def create_dataset_impl(tx_list,rx_list,capture_date_list,max_sig=None,equalized_list=[0],full_dataset_path = 'data/',op_dataset_file = None):\n",
        "    dataset = {}\n",
        "    dataset['tx_list'] = tx_list\n",
        "    dataset['rx_list'] = rx_list\n",
        "    dataset['capture_date_list']=capture_date_list\n",
        "    dataset['equalized_list'] = equalized_list\n",
        "    dataset['max_sig'] = max_sig\n",
        "    \n",
        "    n_tx = len(tx_list)\n",
        "    n_rx = len(rx_list)\n",
        "    n_day = len(capture_date_list)\n",
        "    n_eq = len(equalized_list)\n",
        "    \n",
        "    prefix_lut = [None,'eq']\n",
        "    \n",
        "    prefix_list = [prefix_lut[tt] for tt in  equalized_list]\n",
        "    \n",
        "    dataset['data'] = [ [ [ [ [ ] for _ in range(n_eq)] for _ in range(n_day) ] for _ in range(n_rx) ]  for _ in range(n_tx)     ]\n",
        "    \n",
        "    \n",
        "    missing_rx_dict = {}\n",
        "    \n",
        "    missing_files = False\n",
        "\n",
        "    \n",
        "    with open('IdSig_info.pkl','rb') as f:\n",
        "        IdSig_info=pickle.load(f)\n",
        "    \n",
        "    slc = slice(None,max_sig)\n",
        "    for day_i,capture_date in enumerate(capture_date_list):\n",
        "        for rx_i,rx_train in enumerate(rx_list):\n",
        "            for eq_i,prefix in enumerate(prefix_list):\n",
        "                tdataset = load_from_full_dataset(full_dataset_path,capture_date,rx_train,prefix=prefix)\n",
        "                if not tdataset is None:\n",
        "                    for tx_i,tx in enumerate(tx_list):\n",
        "                        if tx in tdataset['node_list']:\n",
        "                            tx_indx = tdataset['node_list'].index(tx)\n",
        "                            dataset['data'][tx_i][rx_i][day_i][eq_i]= tdataset['data'][tx_indx][slc]  \n",
        "                        else:\n",
        "                            dataset['data'][tx_i][rx_i][day_i][eq_i]=np.zeros((0,256,2))\n",
        "                else:\n",
        "                    missing_rx_name =rx_list[rx_i]  \n",
        "                    eq_val = equalized_list[eq_i]\n",
        "                    IdSig_info_sub  = IdSig_info[eq_val][capture_date]\n",
        "                    if missing_rx_name  in IdSig_info_sub.keys():\n",
        "                            missing_files = True\n",
        "                            if not eq_val in  missing_rx_dict.keys():\n",
        "                                missing_rx_dict[eq_val]={}\n",
        "                            if not capture_date in  missing_rx_dict[eq_val].keys():\n",
        "                                missing_rx_dict[eq_val][capture_date]=[]\n",
        "                            missing_rx_info  = IdSig_info_sub[missing_rx_name]\n",
        "                            missing_rx_dict[eq_val][capture_date].append(   (missing_rx_info['name'], missing_rx_info['link'],missing_rx_info['size']) )\n",
        "\n",
        "    \n",
        "    if missing_files:\n",
        "        ii=1\n",
        "        total_file_sizes = 0\n",
        "        print('You have missing files that you need to download.')\n",
        "        \n",
        "        for eq_k  in missing_rx_dict.keys():  \n",
        "            if len(missing_rx_dict[eq_val])>0:\n",
        "                print('')\n",
        "                if eq_k==0:\n",
        "                    print('You need to download the following files for the non equalized dataset')\n",
        "                else:\n",
        "                    print('You need to download the following files for the equalized dataset')\n",
        "                \n",
        "                print('')\n",
        "                \n",
        "                for date_k  in missing_rx_dict[eq_k].keys():  \n",
        "                    for missing_rx in missing_rx_dict[eq_val][date_k]:\n",
        "                        print('{}) Name: {} , Size: {} MB'.format(ii,missing_rx[0],missing_rx[2]/1e6))\n",
        "                        total_file_sizes=total_file_sizes+missing_rx[2]\n",
        "                        ii=ii+1\n",
        "                print('Links:')\n",
        "                for date_k  in missing_rx_dict[eq_k].keys():  \n",
        "                    for missing_rx in missing_rx_dict[eq_val][date_k]:\n",
        "                        print('https://drive.google.com/u/0/uc?export=download&id={}'.format(missing_rx[1]))               \n",
        "        print('')\n",
        "        print('You need to dowlnoad {} GB'.format(total_file_sizes/1e9))\n",
        "        print('Note the following:')\n",
        "        print('1) The non-equalized and eqalized files need to be downloaded in different fodlers because they share the same exact names')\n",
        "        print('2) The  non-equalized folders needs to be grouped by date and equalization using the same structure as the following google drive folder')\n",
        "        print('https://drive.google.com/drive/folders/1r8cd4zZ7fwvN_iiyI_uDKbIFGZve49lw?usp=sharing')\n",
        "        print('3) If you have already downloaded the files make sure that the full dataset path is configured correctly.')\n",
        "        dataset = None\n",
        "    else:\n",
        "        if not op_dataset_file is None:\n",
        "            with open(op_dataset_file,'wb') as f:\n",
        "                pickle.dump(dataset,f)\n",
        "                print('Dataset saved in {}'.format(op_dataset_file))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def merge_compact_dataset(compact_dataset,capture_date,tx_list,rx_list,max_sig=None,equalized=0):\n",
        "    dataset = {}\n",
        "    dataset['node_list'] = tx_list\n",
        "    dataset['data'] = [ () for _ in range(len(tx_list))]\n",
        "    \n",
        "    if not type(capture_date) is list: \n",
        "        capture_date_list = [capture_date]\n",
        "    else:\n",
        "        capture_date_list = capture_date\n",
        "    slc = slice(None,max_sig)\n",
        "    for capture_date in capture_date_list:\n",
        "        for rx_train in rx_list:\n",
        "            for indx,tx in enumerate(tx_list):\n",
        "                tx_i=compact_dataset['tx_list'].index(tx)\n",
        "                rx_i=compact_dataset['rx_list'].index(rx_train)\n",
        "                date_i=compact_dataset['capture_date_list'].index(capture_date)\n",
        "                eq_i=compact_dataset['equalized_list'].index(equalized)\n",
        "                dataset['data'][indx]  +=  (compact_dataset['data'][tx_i][rx_i][date_i][eq_i][slc],)\n",
        "    for indx in range(len(tx_list)):\n",
        "        if len(dataset['data'][indx])>0:\n",
        "            dataset['data'][indx] =  np.concatenate(dataset['data'][indx])\n",
        "        else:\n",
        "            dataset['data'][indx] =np.zeros((0,256,2))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "6U6A7mWsxYPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "AHdL7PFfxieU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ioakfsp4teKh",
        "outputId": "5bd496d3-fc6e-405f-cf0d-58d0f03c1818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import scipy,scipy.spatial\n",
        "import matplotlib\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "!pip install torch-hd\n",
        "\n",
        "from torchhd import functional\n",
        "from torchhd import embeddings\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVmFFTIRZuaL",
        "outputId": "e5cf5eab-704e-40cb-bbdf-f6e2ea2d10c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-hd\n",
            "  Downloading torch_hd-4.5.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from torch-hd) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-hd) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from torch-hd) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-hd) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-hd) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.0->torch-hd) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torch-hd) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torch-hd) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-hd) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-hd) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-hd) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-hd) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->torch-hd) (1.15.0)\n",
            "Installing collected packages: torch-hd\n",
            "Successfully installed torch-hd-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/ManyRx.pkl.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ],
      "metadata": {
        "id": "REjUQSlQu9Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_compact_pkl_dataset(dataset_path,dataset_name):\n",
        "    with open(dataset_path+dataset_name+'.pkl','rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    return dataset\n",
        "\n",
        "compact_dataset = load_compact_pkl_dataset(\"./\",\"ManyRx\")\n",
        "\n",
        "tx_list = compact_dataset['tx_list']\n",
        "rx_list = compact_dataset['rx_list']\n",
        "\n",
        "equalized = 0\n",
        "\n",
        "capture_date_list = compact_dataset['capture_date_list']\n",
        "capture_date = capture_date_list[0]\n",
        "n_tx = len(tx_list)\n",
        "n_rx = len(rx_list)\n",
        "print(n_tx,n_rx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06y2qsqnuasY",
        "outputId": "10960f3a-0dab-4f6b-9da9-2a628d37734d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compact_dataset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWiI8yA2vcUc",
        "outputId": "2c05601e-8bb1-40a6-cce6-1b77c4941c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['tx_list', 'rx_list', 'capture_date_list', 'equalized_list', 'max_sig', 'data'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "n_real = 5\n",
        "rx_list_real = []\n",
        "for i in range(n_real):\n",
        "    np.random.shuffle(rx_list)\n",
        "    rx_list_real.append(np.copy(rx_list).tolist())\n",
        "print(rx_list_real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ikdXBivkhR",
        "outputId": "7d211745-6c35-425f-accf-600bfbe6f8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['19-20', '24-13', '19-2', '1-20', '20-20', '20-1', '7-7', '3-19', '23-6', '2-19', '24-5', '14-7', '23-1', '19-1', '8-7', '24-6', '24-16', '1-19', '8-8', '18-19', '13-7', '23-3', '8-14', '23-5', '19-19', '18-2', '7-14', '13-14', '1-1', '23-7', '20-19', '2-1'], ['1-1', '23-1', '24-6', '8-8', '1-19', '23-3', '23-5', '7-14', '19-19', '13-14', '23-6', '7-7', '19-1', '20-1', '20-20', '24-16', '8-14', '19-2', '14-7', '1-20', '13-7', '24-5', '18-2', '2-19', '24-13', '19-20', '3-19', '8-7', '20-19', '2-1', '23-7', '18-19'], ['24-5', '23-7', '18-19', '23-3', '24-6', '8-14', '2-1', '13-7', '19-19', '19-2', '18-2', '1-20', '20-1', '24-16', '3-19', '1-19', '24-13', '23-6', '19-1', '8-7', '20-19', '1-1', '14-7', '20-20', '7-7', '2-19', '23-5', '8-8', '19-20', '13-14', '7-14', '23-1'], ['1-20', '1-19', '20-19', '23-7', '2-1', '20-1', '19-19', '14-7', '23-1', '3-19', '8-8', '7-7', '13-7', '20-20', '24-16', '18-2', '8-7', '23-5', '7-14', '18-19', '24-6', '19-1', '13-14', '2-19', '19-20', '24-5', '23-3', '19-2', '8-14', '23-6', '24-13', '1-1'], ['18-19', '20-1', '13-14', '18-2', '14-7', '20-20', '13-7', '19-20', '2-19', '19-19', '19-1', '1-20', '24-5', '20-19', '8-7', '23-1', '7-7', '8-8', '2-1', '1-19', '3-19', '23-3', '23-6', '23-5', '24-6', '8-14', '24-16', '7-14', '1-1', '19-2', '24-13', '23-7']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "import keras.models as models\n",
        "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
        "from keras.layers.noise import GaussianNoise\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.regularizers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import _pickle as cPickle, random\n",
        "import sys, keras"
      ],
      "metadata": {
        "id": "uFJMnaORaGlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small eda (two rp are different?)"
      ],
      "metadata": {
        "id": "Bm4TdC4nUOx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(sig_dfTest[0].reshape((1,-1)).astype(\"float32\"))\n",
        "w_init = tf.random_normal_initializer(stddev=0.01)\n",
        "w = tf.Variable(\n",
        "            initial_value=w_init(shape=(512, 10), dtype=\"float32\"),\n",
        "            trainable=False,\n",
        ")\n",
        "b = tf.random.uniform(shape = (1,10),minval=0,maxval=2*math.pi)\n",
        "print(tf.matmul(inputs, w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG8fZ2UgPO6L",
        "outputId": "519c42ba-808e-430f-9cb5-c8b9c6d637cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.12629473 -0.17342538  0.15582472  0.07324233  0.07992842 -0.06330156\n",
            "   0.19764157 -0.1545074   0.02438615 -0.00329605]], shape=(1, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine = tf.math.cos((tf.matmul(inputs, w)+tf.ones([len(inputs), 1]) * b))\n",
        "sine =tf.math.sin(tf.matmul(inputs, w))\n",
        "print(cosine*sine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygjq1kirP9Tv",
        "outputId": "f74218a2-bdf6-4a33-9399-0e59f87e1cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.0071905  -0.15452248 -0.10651321 -0.06701374  0.0385955  -0.04898038\n",
            "  -0.03035979  0.10064126  0.0240202   0.00279104]], shape=(1, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sig_dfTest[5][:,0],sig_train[0][:,1],'bo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "bJ_2wU--TdGr",
        "outputId": "3cdc2815-b70a-48b8-e5f4-67f36661fba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f643a6f59d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HD"
      ],
      "metadata": {
        "id": "6uxCA8REURSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RP(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32, batch_size =1024, std=0.01):\n",
        "        super(RP, self).__init__()\n",
        "        w_init = tf.random_normal_initializer(stddev=std)\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=False,\n",
        "        )\n",
        "        self.b = tf.random.uniform(shape = (1,units),minval=0,maxval=2*math.pi)\n",
        "    def call(self, inputs):\n",
        "        #return tf.math.sign(tf.matmul(inputs, self.w))\n",
        "        return tf.matmul(inputs, self.w)\n",
        "        \n",
        "        #cosine = tf.math.cos((tf.matmul(inputs, self.w)+tf.ones([len(inputs), 1]) * self.b))\n",
        "        #sine =tf.math.sin(tf.matmul(inputs, self.w))\n",
        "        #return  cosine*sine\n",
        "        \n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config(a).copy()\n",
        "        config.update({\n",
        "            'matrix': self.w.numpy()\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "AkYE-mkxZ_LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classHV_real = np.zeros((10,10000),dtype = np.float32)\n",
        "classHV = np.zeros((10,10000),dtype = np.float32)\n",
        "lr = 0.1\n",
        "input_size = 256\n",
        "RPL = RP(10000, input_size,std=0.01)\n"
      ],
      "metadata": {
        "id": "YSoSUKc8aLW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nay21tfQw68X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_test_rx = 5"
      ],
      "metadata": {
        "id": "a-kavuVYxD72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf weights\n",
        "!mkdir weights"
      ],
      "metadata": {
        "id": "A3Onf_eF2Yix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_HD(train_loader, classHV_real, classHV, path):\n",
        "  for samples, labels in train_loader:\n",
        "      s_transformed = RPL(tf.reshape(samples, [len(samples),512])).numpy()\n",
        "      #s_transformed = transformer.fit_transform(HDmodel(samples).numpy())\n",
        "      '''\n",
        "      for i in range(len(samples)):\n",
        "          s_transformed[i] = s_transformed[i] / np.linalg.norm(s_transformed[i], ord=1)\n",
        "      '''\n",
        "      for i in range(len(samples)):\n",
        "          classHV_real[np.where(labels[i]==1)[0]]+=s_transformed[i]\n",
        "\n",
        "  for i in range(10):\n",
        "      if np.linalg.norm(classHV_real[i], ord=1)!=0:\n",
        "        classHV[i] = classHV_real[i] / np.linalg.norm(classHV_real[i], ord=1)\n",
        "  '''\n",
        "  for i in range(8):\n",
        "      classHV[i] = np.sign(classHV_real[i])\n",
        "  '''\n",
        "  np.save(path, classHV_real)\n",
        "\n",
        "def load_weights(classHV_real, classH, path):\n",
        "  classHV_real = np.load(path)\n",
        "  for i in range(10):\n",
        "    if np.linalg.norm(classHV_real[i], ord=1)!=0:\n",
        "      classHV[i] = classHV_real[i] / np.linalg.norm(classHV_real[i], ord=1)  "
      ],
      "metadata": {
        "id": "_xQkdvE8dF-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4hfUqb0iBeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def HD_predict(test_input, classHV):\n",
        "    y_hat = []\n",
        "    for i in range(0,len(test_input)):\n",
        "        dis = np.ones((10))\n",
        "        for j in range(10):\n",
        "            #dis[j] = scipy.spatial.distance.hamming(test_input[i], classHV[j])\n",
        "            if np.linalg.norm(classHV[j], ord=1):\n",
        "              dis[j] = scipy.spatial.distance.cosine(test_input[i], classHV[j])\n",
        "        index = np.argmin(dis)\n",
        "        result = np.zeros((10))\n",
        "        result[index]=1\n",
        "        y_hat.append(result)\n",
        "    return np.asarray(y_hat)"
      ],
      "metadata": {
        "id": "T4ypOnnfhU4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test(classHV, test_loader):\n",
        "\n",
        "    test_Y_i_hat = np.zeros((0,10))\n",
        "    test_Y = np.zeros((0,10))\n",
        "    for samples, labels in test_loader:\n",
        "        s_transformed = RPL(tf.reshape(samples, [len(samples),512])).numpy()\n",
        "        y_hat = HD_predict(s_transformed, classHV)\n",
        "        test_Y_i_hat = np.concatenate((test_Y_i_hat, y_hat))\n",
        "        test_Y = np.concatenate((test_Y, labels))\n",
        "    test_Y_i_hat = np.array(test_Y_i_hat)\n",
        "\n",
        "    \n",
        "    acc = np.mean(np.argmax(test_Y_i_hat,1)==np.argmax(test_Y,1))\n",
        "    '''\n",
        "    test_indx = ()\n",
        "    for indx in range(len(tx_list)):\n",
        "        cls_indx = np.where(txidNum_dfTest == indx)\n",
        "        test_indx = test_indx + (cls_indx[0][:n_test_samples],)\n",
        "    test_indx = np.concatenate(test_indx) \n",
        "    acc_bal = np.mean(np.argmax(test_Y_i_hat[test_indx,:],1)==txidNum_dfTest[test_indx])\n",
        "    return acc,acc_bal\n",
        "    '''\n",
        "    return acc"
      ],
      "metadata": {
        "id": "pR03LhIZiCJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf weights\n",
        "!mkdir weights"
      ],
      "metadata": {
        "id": "8qGpg-39Hnh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = True\n",
        "continue_training = True\n",
        "nreal = 5\n",
        "\n",
        "real_list = list(range(nreal))\n",
        "nrx_list =  list(range( 0,len(rx_list_real[0])-n_test_rx+1,5))  # [0,len(rx_list_real[0])-1] #\n",
        "\n",
        "patience = 5\n",
        "n_epochs = 100\n",
        "\n",
        "smTest_results = []\n",
        "dfTest_results = []\n",
        "dfTestBal_results = []\n",
        "\n",
        "for real in real_list:\n",
        "    rx_list = rx_list_real[real]\n",
        "    rx_test_list = rx_list[-n_test_rx:]\n",
        "    test_dataset = merge_compact_dataset(compact_dataset,capture_date,tx_list,rx_test_list)\n",
        "    test_augset_dfRx,_,_ = prepare_dataset(test_dataset,tx_list,val_frac=0.0, test_frac=0.0)\n",
        "\n",
        "    [sig_dfTest,txidNum_dfTest,txid_dfTest,cls_weights] = test_augset_dfRx\n",
        "    \n",
        "\n",
        "    df_test_tensor = torch.from_numpy(sig_dfTest)\n",
        "    dftest_comb = [(df_test_tensor[i],txid_dfTest[i]) for i in range(len(df_test_tensor))]\n",
        "    dftest_loader = DataLoader(dftest_comb, batch_size = 16, shuffle = False)\n",
        "\n",
        "    cnt=np.histogram(txidNum_dfTest,bins=np.arange(len(tx_list)+1)-0.5)\n",
        "    n_test_samples = int(np.min(cnt[0]))\n",
        "\n",
        "    smTest_results_real = []\n",
        "    dfTest_results_real = []\n",
        "    dfTestBal_results_real = []\n",
        "\n",
        "    for nrx in nrx_list:\n",
        "        #init hypervector\n",
        "        classHV_real = np.zeros((10,10000),dtype = np.float32)\n",
        "        classHV = np.zeros((10,10000),dtype = np.float32)\n",
        "        lr = 0.1\n",
        "        input_size = 512\n",
        "        RPL = RP(10000, input_size,std=0.7)\n",
        "\n",
        "        print(\"\");print(\"\")\n",
        "        print(\"nrx: {} - real: {} \".format(nrx,real))\n",
        "        fname_w = 'weights/d003_{:02d}_{:02d}.npy'.format(nrx,real)\n",
        "        rx_train_list= rx_list[:nrx+1]\n",
        "\n",
        "        dataset = merge_compact_dataset(compact_dataset,capture_date,tx_list,rx_train_list)\n",
        "\n",
        "        train_augset,val_augset,test_augset_smRx =  prepare_dataset(dataset,tx_list,\n",
        "                                                            val_frac=0.1, test_frac=0.1)\n",
        "        [sig_train,txidNum_train,txid_train,cls_weights] = train_augset\n",
        "        [sig_valid,txidNum_valid,txid_valid,_] = val_augset\n",
        "        [sig_smTest,txidNum_smTest,txid_smTest,cls_weights] = test_augset_smRx\n",
        "        \n",
        "        spl_weights = np.zeros((len(sig_train)))\n",
        "        for index in range(len(txid_train)):\n",
        "          spl_weights[index] = cls_weights[np.argmax(txid_train[index])]\n",
        "\n",
        "        X_train_tensor = torch.from_numpy(sig_train)\n",
        "        X_valid_tensor = torch.from_numpy(sig_valid)\n",
        "        X_test_tensor = torch.from_numpy(sig_smTest)\n",
        "        #X_train_tensor = X_train_tensor.long()\n",
        "        #X_test_tensor = X_test_tensor.long()\n",
        "        \n",
        "        train_comb = [(X_train_tensor[i],txid_train[i]) for i in range(len(X_train_tensor))]\n",
        "        valid_comb = [(X_valid_tensor[i],txid_valid[i]) for i in range(len(X_valid_tensor))]\n",
        "        test_comb = [(X_test_tensor[i],txid_smTest[i]) for i in range(len(X_test_tensor))]\n",
        "\n",
        "        train_loader = DataLoader(train_comb, batch_size = 16, shuffle = False)\n",
        "        valid_loader = DataLoader(valid_comb, batch_size = 16, shuffle = False)\n",
        "        test_loader = DataLoader(test_comb, batch_size = 16, shuffle = False)\n",
        "        if continue_training:\n",
        "            skip = os.path.isfile(fname_w)\n",
        "        else:\n",
        "            skip = False\n",
        "        #print(sig_train.shape)\n",
        "\n",
        "\n",
        "        # classifier = create_net()\n",
        "        if TRAIN and not skip:\n",
        "            train_HD(train_loader, classHV_real, classHV, fname_w)\n",
        "            '''\n",
        "            filepath = 't_weights_0'\n",
        "            c=[ keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True),\n",
        "              keras.callbacks.EarlyStopping(monitor='val_loss',  patience=patience)]\n",
        "            #history = classifier.fit(sig_train,txid_train,class_weight=cls_weights,validation_data=(sig_valid , txid_valid),callbacks=c, epochs=n_epochs)\n",
        "            history = classifier.fit(sig_train,txid_train,sample_weight=spl_weights,\n",
        "                                     validation_data=(sig_valid , txid_valid),callbacks=c, epochs=n_epochs, verbose=2)\n",
        "            classifier.load_weights(filepath)\n",
        "            classifier.save_weights(fname_w,save_format=\"h5\")\n",
        "            '''\n",
        "        else:\n",
        "            load_weights(classHV_real, classHV, fname_w)\n",
        "          \n",
        "\n",
        "        #smTest_r = classifier.evaluate(sig_smTest,txid_smTest,verbose=0)[1]\n",
        "    #     dfTest_r = classifier.evaluate(sig_dfTest,txid_dfTest)[1]\n",
        "        #dfTest_r,dfTestBal_r = evaluate_test(classifier)\n",
        "\n",
        "        smTest_r = evaluate_test(classHV, test_loader)\n",
        "        dfTest_r = evaluate_test(classHV, dftest_loader)\n",
        "\n",
        "        print(smTest_r,dfTest_r)\n",
        "        smTest_results_real.append(smTest_r)\n",
        "        dfTest_results_real.append(dfTest_r)\n",
        "        #dfTestBal_results_real.append(dfTestBal_r)\n",
        "        #K.clear_session()\n",
        "    smTest_results.append(smTest_results_real)\n",
        "    dfTest_results.append(dfTest_results_real)\n",
        "    #dfTestBal_results.append(dfTestBal_results_real)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "FE952-bIxID7",
        "outputId": "07453765-c1ba-457d-c69e-4ceaf79acb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e7494fe5acc6>:119: RuntimeWarning: invalid value encountered in true_divide\n",
            "  cls_weights = np.max(stat,axis=0)/stat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "nrx: 0 - real: 0 \n",
            "0.19 0.1223\n",
            "\n",
            "\n",
            "nrx: 5 - real: 0 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8d05fd62e22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#dfTest_r,dfTestBal_r = evaluate_test(classifier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msmTest_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassHV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mdfTest_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassHV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdftest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ad548ea1859f>\u001b[0m in \u001b[0;36mevaluate_test\u001b[0;34m(classHV, test_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0ms_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRPL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHD_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassHV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtest_Y_i_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Y_i_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-37155805c307>\u001b[0m in \u001b[0;36mHD_predict\u001b[0;34m(test_input, classHV)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m#dis[j] = scipy.spatial.distance.hamming(test_input[i], classHV[j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassHV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m               \u001b[0mdis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassHV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcosine\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m#   or 'reflective correlation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# clamp the result to 0-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcorrelation\u001b[0;34m(u, v, w, centered)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0muu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Return absolute value to avoid small negative value due to rounding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matplotlib.rcParams['figure.dpi'] = 100\n",
        "plt.errorbar(np.array(nrx_list)+1,np.mean(smTest_results,0),np.std(smTest_results,0),capsize=4)\n",
        "plt.errorbar(np.array(nrx_list)+1,np.mean(dfTest_results,0),np.std(dfTest_results,0),capsize=4)\n",
        "plt.legend(['Same Rx(s)','Diff. Rx'])\n",
        "plt.xlabel('N Train Rx')\n",
        "plt.ylabel('Class. Accuracy')\n",
        "#plt.xticks(range(0,len(nrx_list),2))\n",
        "plt.grid()\n",
        "print(np.mean(dfTest_results,0).tolist())"
      ],
      "metadata": {
        "id": "9nuftguw1wE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/weights /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "SV6l5yVpJcTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}